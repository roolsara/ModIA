{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMMppWbnG3dN"
   },
   "source": [
    "<font size=\"+3\">Auto-encodeurs</font>\n",
    "\n",
    "---\n",
    "\n",
    "L'objectif de ce TP est de manipuler des auto-encodeurs sur un exemple simple : la base de données MNIST. L'idée est de pouvoir visualiser les concepts vus en cours, et notamment d'illustrer la notion d'espace latent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# Chargement et normalisation (entre 0 et 1) des données de la base de données MNIST\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu dans les précédents TPs, la stabilité des algorithmes est améliorée par la normalisation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant affiche des exemples d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "idx = [random.randint(0, x_train.shape[0]) for _ in range(0, n)]\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(x_train[idx[i]].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.grid(False)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un premier auto-encodeur **très** simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord construire une architecture très simple où :\n",
    "\n",
    "* L'**encodeur** : est une couche dense composée de 32 neurones (la dimension de la variable latente) avec une fonction d'activation $\\texttt{reLu}$ :\n",
    "$$\\texttt{reLu}(x) = max(0,x) \\,,$$\n",
    "\n",
    "* Le **décodeur** : est une couche dense composée de $784=28\\times28$ neurones (la dimension d'entrée) avec une fonction d'activation sigmoïde\n",
    "$$\\sigma(x) = \\frac{1}{1+\\text{e}^x} \\,.$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par vectoriser les images d'entrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation des images d'entrée en vecteurs de dimmension 784\n",
    "x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "n_latent = 32\n",
    "n_input = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : écrire le modèle simple décrit ci-dessus dans `Keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/simple_autoencoder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons à présent entraîner le modèle. Notez que _la variable cible est l'image originale._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "simple_autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque** : Nous avons utilisé ici l'entropie croisée binaire comme fonction de perte en suivant l'article fondateur [[Kingma & Welling, 2014]](https://arxiv.org/pdf/1312.6114.pdf). Ce choix est ici licite du fait de la fonction d'activation _sigmoïde_.\n",
    "\n",
    "**Exercice** : Vérifier la performance du modèle en visualisant des exemples d'images de la base de test et de leur reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "def vizualise (x_test,imgs,sz=(28,28)) :\n",
    "    # sz désigne la taille des images 'imgs' avant reshape pour l'apprentissage\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/vizualise.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = simple_autoencoder.predict(x_test)\n",
    "vizualise (x_test,decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un auto-encodeur plus réaliste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de pouvoir accéder facilement à l'espace latent, on préfère généralement définir trois modèle distinct : un encodeur, un décodeur et l'auto-encodeur comme concaténation des deux.\n",
    "\n",
    "Le code ci-après permet cette décomposition. On au profite pour complexifier _légèrement_ l'architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latent = 32\n",
    "n_input = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension de l'entrée\n",
    "input_img = Input(shape=(n_input,))\n",
    "\n",
    "# Définition de l'encoder\n",
    "x = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(n_latent, activation='linear')(x)\n",
    "encoder = Model(input_img, encoded, name = \"encoder\")\n",
    "\n",
    "# Définition du decoder\n",
    "decoder_input = Input(shape=(n_latent,))\n",
    "x = Dense(128, activation='relu')(decoder_input)\n",
    "decoded = Dense(784, activation='linear')(x)\n",
    "decoder = Model(decoder_input, decoded, name = \"decoder\")\n",
    "\n",
    "# Construction de l'auto-encodeur\n",
    "encoded = encoder(input_img)\n",
    "decoded = decoder(encoded)\n",
    "autoencoder = Model(input_img, decoded, name = \"autoencoder\")\n",
    "\n",
    "print(autoencoder.summary())\n",
    "print(\" \")\n",
    "print(encoder.summary())\n",
    "print(\" \")\n",
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "vizualise(x_test,decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Alors qu'on a complexifié le modèle, on obtient des résultats moins probants. À votre avis, à quoi cela peut-il être dû ? \n",
    "En vous aidant du premier modèle proposé, proposez une amélioration permettant d'obtenir de meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "def build_autoencoder(n_latent = 32):\n",
    "    # n_latent = dimension de l'espace latent\n",
    "    \n",
    "    [...]\n",
    "\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/build_autoencoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder, encoder, decoder = build_autoencoder(n_latent)\n",
    "autoencoder.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "  epochs=50,\n",
    "  batch_size=128,\n",
    "  shuffle=True,\n",
    "  validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "vizualise(x_test,decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque** : La décomposition encodeur/décodeur dans la définition de l'auto-encodeur permet de visualiser facilement l'espace latent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(x_test)\n",
    "encoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualise(x_test,encoded_imgs,sz=(4,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Vérifier que vous obtenez les résultats en décodant les représentations latentes qu'en codant-décodant les données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "vizualise(autoencoded_imgs,decoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "idx = [random.randint(0, x_test.shape[0]) for _ in range(0, n)]\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(4, n, i+1)\n",
    "    plt.imshow(x_test[idx[i]].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display encoded\n",
    "    ax = plt.subplot(4, n, i+1 + n)\n",
    "    plt.imshow(encoded_imgs[idx[i]].reshape(4,8))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction through auto-encoder\n",
    "    ax = plt.subplot(4, n, i+1 + 2*n)\n",
    "    plt.imshow(autoencoded_imgs[idx[i]].reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display decoded\n",
    "    ax = plt.subplot(4, n, i+1 + 3*n)\n",
    "    plt.imshow(decoded_imgs[idx[i]].reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "title = ['test data','endoded','reconstruction','decoded']\n",
    "for i in range(4) :  \n",
    "    ax = plt.subplot(4,n,1+i*n)\n",
    "    plt.title(title[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence de la dimension de l'espace latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la dimension d'espace latent qui vous est fournie, on observe une (relativement) faible erreur de reconstruction. On souhaite étudier l'influence de la dimension de l'espace latent sur l'erreur de reconstruction.\n",
    "\n",
    "**Exercice** : Tracez une _courbe_ (avec seulement quelques points) qui montre l'évolution de l'erreur de reconstruction en fonction de la dimension de l'espace latent. Quelle est la dimension minimale de l'espace latent qui permet encore d'observer une reconstruction raisonnable des données (avec le réseau qui vous est fourni) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load dim_latent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice (pour aller plus loin)** : Pour diminuer encore plus la dimension de l'espace latent, il est nécessaire d'augmenter la capacité des réseaux encodeur et décodeur. Cherchez à nouveau la dimension minimale de l'espace latent qui permet d'observer une bonne reconstruction des données, mais en augmentant à l'envi la capacité de votre auto-encodeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-encodeurs éparses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exemple précédent, l'autoencodeur n'est contraint que par la taille de la couche cachée.\n",
    "\n",
    "Dans la figure suivante, vous pouvez voir la distribution du nombre de variables latentes fixées à zéro pour les 10.000 images de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.hist(np.sum(encoded_imgs==0,axis=1), width=0.9, bins=np.arange(-0.5,10.5,1))\n",
    "ax.set_xticks(np.arange(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autrement dit, notre espace latent est très plein... On désire obtenir une représentations latente _creuse_. Pour cela, nous allons ajouter à notre auto-encodeur une contrainte de parcimonie.\n",
    "\n",
    "**Exercice** : En modifiant l'auto-encodeur précédent, réaliser un encodage parcimonieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "n_latent = 32\n",
    "l = 10e-4\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/build_sparse_autoencoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparse_autoencoder.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "\n",
    "sparse_autoencoder.fit(x_train, x_train,\n",
    "  epochs=50,\n",
    "  batch_size=256,\n",
    "  #shuffle=True,\n",
    "  validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Vérifiez que les images encodées par l'auto-codeur parcimonieux sont effectivement plus creuses que celles obtenues par l'auto-codeur classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sparse_autoencoder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Comparez les images reconstruites obtenues par le premier modèle et le modèle parcimonieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sparse_autoencoder_compare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-encodeurs convolutionnels\n",
    "\n",
    "Dans les parties précédentes, nous avons vu des auto-codeurs très simples où les parties encodeur et décodeur sont des perceptrons. Comme vu en cours, ils peuvent tous deux être composés de plus de couches et de différents types de couches.\n",
    "\n",
    "En particulier, les couches convolutionnelles sont les meilleures couches à utiliser lorsqu'il s'agit d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Implémentez un auto-codeur convolutif avec l'architecture suivante : \n",
    "\n",
    "`Encodeur` :\n",
    "* Une couche de convolution 2D, 16 filtres de taille 3x3\n",
    "* Une couche de maxpooling 2D avec des filtres de taille 2x2\n",
    "* Une couche de convolution 2D, 8 filtres de taille 3x3\n",
    "* Une couche de maxpooling 2D avec des filtres de taille 2x2\n",
    "* Une couche de convolution 2D, 8 filtres de taille 3x3\n",
    "* Une couche de maxpooling 2D avec des filtres de taille 2x2\n",
    "\n",
    "`Décodeur` :\n",
    "* Une couche de convolution 2D, 8 filtres de taille 3x3\n",
    "* Une couche de suréchantillonnage (upsampling) 2D avec des filtres de taille 2x2\n",
    "* Une couche de convolution 2D, 8 filtres de taille 3x3\n",
    "* Une couche de suréchantillonnage 2D avec des filtres de taille 2x2\n",
    "* Une couche de convolution 2D, 16 filtres de taille 3x3\n",
    "* Une couche de suréchantillonnage 2D avec des filtres de taille 2x2\n",
    "* Une couche de convolution 2D, 1 filtre de taille 3x3, avec activation $\\texttt{sigmoid}$.\n",
    "\n",
    "\n",
    "_Tous les rembourrages (padding) sont des rembourrages $\\texttt{same}$ et toutes les fonctions d'activation des couches de convolution, sauf la dernière, sont des $\\texttt{reLu}$._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez obtenir le summary suivant :\n",
    "\n",
    "```\n",
    "Model: \"conv_autoencoder\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " conv_encoder (Sequential)   (None, 4, 4, 8)           1904      \n",
    "                                                                 \n",
    " conv_decoder (Sequential)   (None, 28, 28, 1)         2481      \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 4,385\n",
    "Trainable params: 4,385\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/build_conv_autoencoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder = build_conv_autoencoder()\n",
    "conv_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.expand_dims(x_train, axis=-1)\n",
    "# x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "\n",
    "conv_autoencoder.fit(x_train, x_train,\n",
    "  epochs=10,\n",
    "  batch_size=256,\n",
    "  validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder.evaluate(x_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_decoded_imgs = conv_autoencoder.predict(x_test)\n",
    "vizualise(x_test,conv_decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application au débruitage\n",
    "\n",
    "Nous savons maintenant comment construire un auto-encodeur convolutionnel. Nous allons maintenant voir comment il peut être utilisé pour résoudre un problème de débruitage d'image.\n",
    "\n",
    "Nous créons d'abord de fausses données bruitées à partir des données MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random noise\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "# Value greater than 1 are set to 1 and value lower than 0 are set to zero\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons le bruit que nous avons créé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualise(x_test,x_test_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : En utilisant l'auto-encodeur défini précédemment, réaliser le débruitage de ces images et visualiser l'effet du débruitage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/denoising.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/denoising_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pour aller plus loin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1** : Écrivez une fonction qui, étant donnés deux données de votre espace de test $I_1$ et $I_2$, réalise l'interpolation (avec, par exemple, 10 étapes) entre la représentation latente ($z_1 = $encoder($I_1$) et $z_2 = $encoder($I_2$)) de ces deux données, et génère les images $I_i$ correspondant aux représentations latentes intermédiaires $z_i$. En pseudo python, cela donne : \n",
    "\n",
    "```python\n",
    "for i in range(10):\n",
    "    z_i = z1 + i*(z2-z1)/10\n",
    "    I_i = decoder(z_i)\n",
    "```\n",
    "Testez cette fonction avec un auto-encodeur avec une faible erreur de reconstruction, sur deux données présentant le même chiffre écrit différemment, puis deux chiffres différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercice 2 (Visage)** : Le code fourni dans la suite permet de télécharger et de préparer une [base de données de visages](http://vis-www.cs.umass.edu/lfw/). \n",
    "\n",
    "ATTENTION : ici les images sont couleur et comportent 3 canaux (contrairement aux images de MNIST, qui n'en comptent qu'un). \n",
    "\n",
    "Par analogie avec l'exercice précédent, on pourrait grâce à la représentation latente apprise par un auto-encodeur, réaliser un morphing entre deux visages. Essayez d'abord d'entraîner un auto-encodeur à obtenir une erreur de reconstruction faible. Qu'observe-t-on ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile, tqdm, cv2, os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Télécharger les données de la base de données \"Labelled Faces in the Wild\"\n",
    "!wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt\n",
    "!wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
    "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
    "  \n",
    "ATTRS_NAME = \"lfw_attributes.txt\"\n",
    "IMAGES_NAME = \"lfw-deepfunneled.tgz\"\n",
    "RAW_IMAGES_NAME = \"lfw.tgz\"\n",
    "\n",
    "\n",
    "def decode_image_from_raw_bytes(raw_bytes):\n",
    "    img = cv2.imdecode(np.asarray(bytearray(raw_bytes), dtype=np.uint8), 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_lfw_dataset(use_raw=False,\n",
    "                     dx=80, dy=80,\n",
    "                     dimx=45, dimy=45):\n",
    "\n",
    "    # Read attrs\n",
    "    df_attrs = pd.read_csv(ATTRS_NAME, sep='\\t', skiprows=1)\n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:, :-1].values, columns=df_attrs.columns[1:])\n",
    "    imgs_with_attrs = set(map(tuple, df_attrs[[\"person\", \"imagenum\"]].values))\n",
    "\n",
    "    # Read photos\n",
    "    all_photos = []\n",
    "    photo_ids = []\n",
    "\n",
    "    # tqdm in used to show progress bar while reading the data in a notebook here, you can change\n",
    "    # tqdm_notebook to use it outside a notebook\n",
    "    with tarfile.open(RAW_IMAGES_NAME if use_raw else IMAGES_NAME) as f:\n",
    "        for m in tqdm.tqdm_notebook(f.getmembers()):\n",
    "            # Only process image files from the compressed data\n",
    "            if m.isfile() and m.name.endswith(\".jpg\"):\n",
    "                # Prepare image\n",
    "                img = decode_image_from_raw_bytes(f.extractfile(m).read())\n",
    "\n",
    "                # Crop only faces and resize it\n",
    "                img = img[dy:-dy, dx:-dx]\n",
    "                img = cv2.resize(img, (dimx, dimy))\n",
    "\n",
    "                # Parse person and append it to the collected data\n",
    "                fname = os.path.split(m.name)[-1]\n",
    "                fname_splitted = fname[:-4].replace('_', ' ').split()\n",
    "                person_id = ' '.join(fname_splitted[:-1])\n",
    "                photo_number = int(fname_splitted[-1])\n",
    "                if (person_id, photo_number) in imgs_with_attrs:\n",
    "                    all_photos.append(img)\n",
    "                    photo_ids.append({'person': person_id, 'imagenum': photo_number})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "    all_photos = np.stack(all_photos).astype('uint8')\n",
    "\n",
    "    # Preserve photo_ids order!\n",
    "    all_attrs = photo_ids.merge(df_attrs, on=('person', 'imagenum')).drop([\"person\", \"imagenum\"], axis=1)\n",
    "\n",
    "    return all_photos, all_attrs\n",
    "\n",
    "\n",
    "# Prépare le dataset et le charge dans la variable X\n",
    "X, attr = load_lfw_dataset(use_raw=True, dimx=32, dimy=32)\n",
    "\n",
    "# Normalise les images\n",
    "X = X/255\n",
    "\n",
    "# Sépare les images en données d'entraînement et de test\n",
    "X_train, X_test = train_test_split(X, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "IAM2020 - TP Autoencodeurs.ipynb",
   "provenance": [
    {
     "file_id": "1gzJrRXgM5UnrQ-0TcDORLh-nf9mPe7e3",
     "timestamp": 1608145618791
    },
    {
     "file_id": "1NEuWUdI_RE21-HSnM97Gaiu7r0mgOpzS",
     "timestamp": 1575908111833
    },
    {
     "file_id": "1W6yojd2fpYsDP2mnH_pbyFF8DzormXTA",
     "timestamp": 1575458798048
    },
    {
     "file_id": "1MhBkPZTpU52Jf2Y50ZAMG0kO6e4mS3Il",
     "timestamp": 1574780203309
    },
    {
     "file_id": "1nWQYnWfLw5xvcASlIHosaJWbXPhDIEG1",
     "timestamp": 1574515967280
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "369a4810aadc4e078a386f1248015e01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f5953fa34c84d80b1b541361cafb82a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "586da8d03b72421fa918afb58aa7be9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a9c121687254c909000d40a029e53d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6801337827042a0b3f6f4a0b6029809",
      "placeholder": "​",
      "style": "IPY_MODEL_6cac2567bdab4333a2783a93e77fc037",
      "value": "100%"
     }
    },
    "6cac2567bdab4333a2783a93e77fc037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "899378ae9d4d4339a221889956807090": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a9c121687254c909000d40a029e53d8",
       "IPY_MODEL_c6b1e9649ddd4a9fb1660f636fc61062",
       "IPY_MODEL_a5aba2b60f4f4f8798f69c4736a6047b"
      ],
      "layout": "IPY_MODEL_369a4810aadc4e078a386f1248015e01"
     }
    },
    "a5aba2b60f4f4f8798f69c4736a6047b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb448b411d1c42b8bc43849a396db319",
      "placeholder": "​",
      "style": "IPY_MODEL_586da8d03b72421fa918afb58aa7be9d",
      "value": " 18983/18983 [00:16&lt;00:00, 1490.04it/s]"
     }
    },
    "a6801337827042a0b3f6f4a0b6029809": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3cc4a10b58f479399cc60ee886ea3d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6b1e9649ddd4a9fb1660f636fc61062": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3cc4a10b58f479399cc60ee886ea3d6",
      "max": 18983,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4f5953fa34c84d80b1b541361cafb82a",
      "value": 18983
     }
    },
    "cb448b411d1c42b8bc43849a396db319": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
