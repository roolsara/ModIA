{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50eb139-5964-4b5a-af0e-e4b69e1560aa",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "_Sentiment analysis through Recurrent Neural Networks_\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, we are interested in the problem of sentiment analysis. In the first part, we will build a recurrent network on a toy dataset from scratch to determine if a sentence is positive or negative. In a second step, using the [`Keras`](https://keras.io/) API, we will build a network able to determine if a movie review is positive or negative.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef5871b-799b-40e5-a2b0-863bfcf6e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7b446-678c-45fe-80e4-a49c8448f3e2",
   "metadata": {},
   "source": [
    "---\n",
    "# **PART I**: RNN from Scratch\n",
    "\n",
    "In order to understand recurrent networks in more detail, our first example will be implementing a network from scratch. The network will perform a (simple) sentiment analysis task, namely determining whether a given text string is positive or negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a36b5-ac50-4a26-9b14-e7a61017eac8",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "The commands below allow displaying some samples of our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73ff428-67f1-4538-89e2-e254c86e0051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', True),\n",
       " ('bad', False),\n",
       " ('happy', True),\n",
       " ('sad', False),\n",
       " ('not good', False),\n",
       " ('not bad', True),\n",
       " ('not happy', False),\n",
       " ('not sad', True),\n",
       " ('very good', True),\n",
       " ('very bad', False),\n",
       " ('very happy', True),\n",
       " ('very sad', False),\n",
       " ('i am happy', True),\n",
       " ('this is good', True),\n",
       " ('i am bad', False)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data import train_data, test_data\n",
    "\n",
    "display( list(train_data.items())[:15] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170f3e9-18b4-421d-a112-fb06cf127bf1",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "In order to visualize quickly the labels, we want display in _green_ the <span style=\"color:green\">positive sentences</span>, and in _red_ the <span style=\"color:orangered\">negative sentences</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f371d9b-de65-4f33-9a2c-2f482ccaa94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c949fe-5a45-461c-99f7-8baba3fc7528",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Using the command `Fore.COLOR` of the package [`colorama`](https://pypi.org/project/colorama/), realize such a function.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97f973db-c41a-47b7-909e-7e3b7e880778",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "def coloredSentences(sentences, out=15):\n",
    "    \"\"\"\n",
    "    Display in green the positive sentences, and in red the negative sentences\n",
    "    - sentences is a dict\n",
    "        - sentences.keys() are the sentences to display\n",
    "        - sentences.values() are booleans that encode the sentiment\n",
    "    - out is an integer indicating the maximum number of sentences to display\n",
    "    \"\"\"\n",
    "    incr = 0\n",
    "    for cle, valeur in sentences.items():\n",
    "        if valeur:\n",
    "            print(Fore.GREEN + cle)   \n",
    "        else:\n",
    "            print(Fore.RED + cle)   \n",
    "        incr += 1\n",
    "        if incr == out:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ac11a88-34b0-4c1c-aa8b-9873947646e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/coloredSentences.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "983c0401-f9c9-44ab-9ef7-5b4f7bce72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mgood\n",
      "\u001b[31mbad\n",
      "\u001b[32mhappy\n",
      "\u001b[31msad\n",
      "\u001b[31mnot good\n",
      "\u001b[32mnot bad\n",
      "\u001b[31mnot happy\n",
      "\u001b[32mnot sad\n",
      "\u001b[32mvery good\n",
      "\u001b[31mvery bad\n",
      "\u001b[32mvery happy\n",
      "\u001b[31mvery sad\n",
      "\u001b[32mi am happy\n",
      "\u001b[32mthis is good\n",
      "\u001b[31mi am bad\n"
     ]
    }
   ],
   "source": [
    "coloredSentences(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bcbe8-1af8-4528-8f3c-52851ed1be38",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "The datasets consists of two $\\texttt{dictionaries}$. Before trying to classify these sentences, we will build a vocabulary of all of all words that exist in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f59b2-9f2c-49cc-a18c-7226ab2b39d9",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** How many different words are in our vocabulary?</span>\n",
    "\n",
    "To answer this question, start by building a **vocabulary**, _i.e._ a $\\texttt{list}$ containing all the words used in the dataset. _Each word should occur only once_.\n",
    "\n",
    "<!-- 18 unique words found -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64de6830-7e4a-4595-bb09-deda480b5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "\n",
    "for key in train_data.keys():\n",
    "    words = key.split()\n",
    "    unique_words.update(words)\n",
    "\n",
    "unique_word_list = list(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93d8981f-29b0-46e7-9ba3-ab5018983886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['or', 'earlier', 'and', 'sad', 'at', 'good', 'is', 'am', 'not', 'very', 'happy', 'all', 'was', 'i', 'right', 'this', 'now', 'bad']\n",
      "--> 18 unique words\n"
     ]
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "vocab = unique_word_list\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print('Vocabulary:',vocab)\n",
    "print('--> %d unique words' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93ea5e36-da35-4d9a-a6a7-789c9646cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/vocab_size.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbee304-e47d-4726-bfa6-f09d81b400fd",
   "metadata": {},
   "source": [
    "### Word Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c1bc7-928c-44c9-8169-1b1b591ecd59",
   "metadata": {},
   "source": [
    "A neural network cannot take strings as input. So we have to encode these sentences in a format understandable by a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215ad08-6dd8-4f4e-9fbd-8534963c45f1",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Assign an integer index to represent each word of the vocab</span>\n",
    "\n",
    "To do that, construct two $\\texttt{dictionaries}$ allowing to translate words into integer indices, and vice versa :\n",
    "\n",
    "* $\\texttt{word\\_to\\_idx}$ has for keys the words of the vocabulary; and for value an integer index, the order in which the words appear in the vocabulary for example.\n",
    "* $\\texttt{idx\\_to\\_word}$ performs the opposite translation: its keys are the integer indices while its values are the associated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80d068ab-58fe-4c73-87e4-a8ff51610c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word \"good\": 5\n",
      "First word in the vocabulary: or\n"
     ]
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "print('Index of word \"good\":', word_to_idx['good'])\n",
    "print('First word in the vocabulary:', idx_to_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8955a2cb-3d3a-4dd4-b2ef-2a7e5962ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/decode.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a93da-4411-446f-bfce-93bf4f47fd51",
   "metadata": {},
   "source": [
    "This way of encoding words works quite well. However, it has the disadvantage of introducing a preferential but meaningless order in how words are processed. Since the vocabulary size is reasonable, we will use a one-shot encoding instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5709bea-e8eb-44e4-ad4a-ec4cdeeb3589",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write a function $\\texttt{createInputs}$ that performs one-hot encoding</span>\n",
    "\n",
    "This function will return a $\\texttt{list}$ of the one-hot encodings of each word that compose the input sentence. Each word is encoded by a vector consisting of zeros and _a_ one at its \"$\\texttt{word\\_to\\_idx}$\" position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f355dd2-3d90-447e-8452-fad744ddbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "def createInputs(text):\n",
    "    '''\n",
    "    Returns an array of one-hot vectors representing the words in the input text string.\n",
    "    - text is a string\n",
    "    - Each one-hot vector has shape (vocab_size, 1)\n",
    "    '''\n",
    "    text_to_tab = text.split()\n",
    "    list_vector = []\n",
    "    for i in text_to_tab:\n",
    "        zeros_vector = np.zeros(vocab_size)\n",
    "        zeros_vector[word_to_idx[i]] = 1\n",
    "        list_vector.append(zeros_vector)\n",
    "    return list_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ce782e44-3e95-4e8d-ad66-c1fc8d750c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/createInputs.py\n",
    "def createInputs(text):\n",
    "    '''\n",
    "    Returns an array of one-hot vectors representing the words in the input text string.\n",
    "    - text is a string\n",
    "    - Each one-hot vector has shape (vocab_size, 1)\n",
    "    '''\n",
    "    inputs = []\n",
    "    for w in text.split(' '):\n",
    "        v = np.zeros((vocab_size, 1))\n",
    "        v[word_to_idx[w]] = 1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c99eedd1-887e-485f-a948-db7b6db5d11d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])]\n"
     ]
    }
   ],
   "source": [
    "print( createInputs('i am very good') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa1677-ef5f-4137-9b0e-2ea557497232",
   "metadata": {},
   "source": [
    "## The Forward Phase\n",
    "\n",
    "In this part, we will build the simplest possible recursive network. To do so, we will create an $\\texttt{RNN}$ class that we will update as we build it. We want to classify a textual data. To do so, we will use a many-to-one network, as shown in the figure below.\n",
    "\n",
    "<img src=\"img/many-to-one.png\" width=250>\n",
    "\n",
    "Let a sentence $x=(x_0,\\ldots,x_n)$, its label $y$, and let $h=(h_0,\\ldots,h_n)$ be the corresponding hidden state. We give ourselves three weight matrices, $W_{xh}$, $W_{hh}$ and $W_{hy}$, and two bias vectors, $b_h$ and $b_y$, so that, for any $t\\in[\\![0,n]\\!]$:\n",
    "\n",
    "$$ \\left\\{\\begin{aligned}\n",
    "    h_t &= \\tanh\\left( W_{xh}x_t + W_{hh}h_{t-1} + b_h \\right) \\\\\n",
    "    y &= softmax(W_{hy}h_n + b_y)\n",
    "\\end{aligned}\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b536e-0edd-4b0f-9bcf-a3c9a8730cc6",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** What is the dimension of the different weight matrices and bias vectors?</span>\n",
    "\n",
    "You can freely use the following notations:\n",
    "* $n_h$ denotes the $\\texttt{hidden\\_size}$, _i.e._ the size oh the hidden vectors $h_t$;\n",
    "* $n_x$ denotes the $\\texttt{input\\_size}$, _i.e._ the size of the inputs $x_t$;\n",
    "* $n_y$ denotes the $\\texttt{output\\_size}$, _i.e._ the size of the output $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81fb7e-25ad-409e-abe7-f69441a3b117",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "W_{xh} = n*n\n",
    "\n",
    "W_{hh} = n*n\n",
    "\n",
    "W_{hy} = n*n\n",
    "\n",
    "b_h = n+1\n",
    "\n",
    "b_y = n+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245a61a-05d5-4608-9a7b-d03bb71bec7c",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "**Solution**:\n",
    "* $W_{xh}\\in\\mathcal{M}_{n_h,n_x}(\\mathbb{R})$\n",
    "* $W_{hh}\\in\\mathcal{M}_{n_h,n_h}(\\mathbb{R})$\n",
    "* $W_{hy}\\in\\mathcal{M}_{n_y,n_h}(\\mathbb{R})$\n",
    "* $b_h\\in\\mathcal{M}_{n_h,1}(\\mathbb{R})$\n",
    "* $b_y\\in\\mathcal{M}_{n_y,1}(\\mathbb{R})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eca9c7-cca2-4147-85e9-a855758a7258",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Initialize the weight matrices and bias vectors. Realize the forward pass.</span>\n",
    "\n",
    "* The weights are initialized from the standard normal distribution, dividing by 1000 to reduce the initial variance. The biases are initialized to zero. \n",
    "* For the forward pass, first initialize the hidden state $h_0$ to zero, then perform each step of the RNN.\n",
    "\n",
    "**Note:** As said, dividing by 1000 the weights reduce the initial variance. This is not the best way to initialize weights, but it's simple and works for this simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04741a3f-df11-4889-b324-c25ae96bd0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "class RNN:\n",
    "    # A Vanilla Recurrent Neural Network.\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)/1000\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size)/1000\n",
    "        self.Why = np.random.randn(output_size, hidden_size)/1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size,1))\n",
    "        self.by = np.zeros((output_size,1))\n",
    "        \n",
    "    # ----- #\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Perform a forward pass of the RNN using the given inputs.\n",
    "        Returns the final output and hidden state.\n",
    "        - inputs is an array of one-hot vectors with shape (input_size, 1).\n",
    "        '''\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        # Perform each step of the RNN\n",
    "        for x in inputs:\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "\n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73c0f0-57c8-457a-963e-f7b00f85cdbb",
   "metadata": {},
   "source": [
    "**Remark:** Before looking at the solution, you can test your $\\texttt{RNN}$ class by passing any input into the network. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b462aae-40a4-4ecd-bb96-7cac866a8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/RNN_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134b449-9af4-4e0f-b69a-2f2109e85e63",
   "metadata": {},
   "source": [
    "The binary classification is performed using the $\\texttt{softmax}$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e382b7-2c91-400d-bae9-d5c4a8c5c21f",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Implement the softmax function.</span>\n",
    "\n",
    "As a reminder, for $x=(x_0,\\ldots,x_n)$ and $i_0\\in[\\![0,n]\\!]$, $~softmax(x_{i_0}) = \\frac{e^{x_{i_0}}}{\\sum_i e^{x_i}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5559d7f-1ce8-4bf6-9b9f-1fe7493cd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "445066ad-64e0-4d8f-8c63-527eb373e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/softmax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b532e5f-8b5f-4729-ade4-fdfb935cb291",
   "metadata": {},
   "source": [
    "To ensure that we have not made an implementation error, we can pass a sentence from the training set through the network. Since the network has not yet been trained, we should find that this sentence is as likely to be positive as negative, i.e., a probability vector approximately equal to [0.5, 0.5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2596c28e-0591-480c-9560-74ef4cc1864b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50000133]\n",
      " [0.49999867]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RNN\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "inputs = createInputs('i am very good')\n",
    "out, _ = rnn.forward(inputs)\n",
    "\n",
    "probs = softmax(out)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4653e2b-af3b-434b-a655-dea00d664948",
   "metadata": {},
   "source": [
    "## The Backward Phase\n",
    "\n",
    "Lets move on to training. To this end, we first need a loss function. We will use the cross-entropy loss, which is often associated with the $softmax$ function. Let $\\sigma$ denotes the $softmax$ function and $y_c$ be the _correct_ class. Then:\n",
    "\n",
    "$$ \\mathcal{L} = \\mathcal{L}(x,y;W_{xh},W_{hh},W_{hy},b_h,b_y) = -\\log(p_c) \\qquad\\text{where}\\qquad p_c = \\sigma(y_c) \\,. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d002d-6db4-4662-b12f-d7dc8cf183a6",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Exercise:** Prove that for all $i\\in[\\![0,n_y]\\!]$, $\\displaystyle\\quad\\frac{\\partial\\mathcal{L}}{\\partial y_i} = \\left\\{\\begin{aligned}\n",
    "    &p_i=\\sigma(y_i) & \\text{if}\\quad i\\neq c\\\\\n",
    "    &p_c-1=\\sigma(y_c)-1 & \\text{if}\\quad i=c\n",
    "\\end{aligned}\\right. $</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b733e7d-b7e9-4edd-ba36-718b24ac3c47",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d048a3-5b1a-4b87-bca1-490f2289ab49",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "**Solution**: $\\mathcal{L}(y_i)=-\\log(\\sigma(y_c))$. Hence, $\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial y_i}=-\\frac1{\\sigma(y_c)}\\times\\frac{\\partial\\sigma}{\\partial y_i}$.\n",
    "* If $i\\neq c$,\n",
    "$$ \\frac{\\partial\\sigma}{\\partial y_i} = \\frac{ -e^{y_c}\\times e^{y_i} }{ \\left(\\sum_ke^{y_k}\\right)^2 }\n",
    "    = \\frac{-e^{y_c}}{\\sum_ke^{y_k}}\\times\\frac{e^{y_i}}{\\sum_ke^{y_k}} = -\\sigma(y_c)\\times\\sigma(y_i) \n",
    "    \\qquad\\text{and}\\qquad\n",
    "   \\frac{\\partial\\mathcal{L}}{\\partial y_i} = -\\frac{-\\sigma(y_c)\\times\\sigma(y_i)}{\\sigma(y_c)} = \\sigma(y_i)=p_i \\,. $$\n",
    "\n",
    "* Else,\n",
    "$$ \\frac{\\partial\\sigma}{\\partial y_c} = \\frac{ e^{y_c}\\left(\\sum_ke^{y_k}\\right)-e^{y_c}\\times e^{y_c} }{ \\left(\\sum_ke^{y_k}\\right)^2 }\n",
    "    = \\frac{e^{y_c}}{\\sum_ke^{y_k}}-\\left(\\frac{e^{y_c}}{\\sum_ke^{y_k}}\\right)^2 = \\sigma(y_c)-\\sigma(y_c)^2 \n",
    "    \\qquad\\text{and}\\qquad\n",
    "   \\frac{\\partial\\mathcal{L}}{\\partial y_c} = -\\frac{\\sigma(y_c)-\\sigma(y_c)^2}{\\sigma(y_c)} = \\sigma(y_c)-1=p_c-1 \\,. $$-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaeab7-29f1-4e11-9904-b443aea5a96a",
   "metadata": {},
   "source": [
    "Let us modify the $\\texttt{forward}$ function in the $\\texttt{RNN}$ class to cache the hidden states $h$ and the inputs $x$, which we will need for computing the gradients in the back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "84b1fdde-aec5-45d9-a7ac-8e7c820e91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # A Vanilla Recurrent Neural Network.\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = rd.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = rd.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = rd.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Perform a forward pass of the RNN using the given inputs.\n",
    "        Returns the final output and hidden state.\n",
    "        - inputs is an array of one-hot vectors with shape (input_size, 1).\n",
    "        '''\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.inputs = inputs  ### NEW ###\n",
    "        self.hs = { 0: h }    ### NEW ###\n",
    "        \n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.hs[i+1] = h  ### NEW ###\n",
    "            \n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def backprop(self, d_y, learn_rate=2e-2):\n",
    "        '''    \n",
    "        Perform a backward pass of the RNN.    \n",
    "        - d_y (dL/dy) has shape (output_size, 1).    \n",
    "        - learn_rate is a float.    \n",
    "        '''    \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc4fff-8b21-436a-88b9-ea81ab890754",
   "metadata": {},
   "source": [
    "Therefore, **given a backward pass**, we can train the RNN using the following loop on all training data. \n",
    "\n",
    "From now on, we will denote $\\frac{\\partial\\mathcal{L}}{\\partial y}$ by $\\texttt{d\\_y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a16f419-b2f2-4d6b-bb8d-cc585c2ecec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.50000317]\n",
      " [ 0.50000317]]\n"
     ]
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "# Loop over each training example\n",
    "for x, y in train_data.items():\n",
    "    inputs = createInputs(x)\n",
    "    target = int(y)\n",
    "\n",
    "    # Forward\n",
    "    out, _ = rnn.forward(inputs)\n",
    "    probs = softmax(out)\n",
    "\n",
    "    # Build dL/dy\n",
    "    d_y = probs\n",
    "    d_y[target] -= 1\n",
    "    \n",
    "    # Backward\n",
    "    rnn.backprop(d_y)\n",
    "    \n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0988a72c-7760-4efc-bfe4-844ba7253568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/trainingLoop.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f23f52-7393-437a-baf6-db2878ec7564",
   "metadata": {},
   "source": [
    "### Gradient Computation\n",
    "\n",
    "It is then sufficient to backpropagate the gradient to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcff619-c716-4da1-a492-0836e2ac198c",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** What are the parameters of the model to optimize?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf18f38-e142-4b00-8598-8638eec82f5a",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c2f99-ef43-4a0d-beca-ebad89fc65c3",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "**Solution**: \n",
    "* The weights matrices $W_{xh}\\in\\mathcal{M}_{n_h,n_x}(\\mathbb{R})$, $W_{hh}\\in\\mathcal{M}_{n_h,n_h}(\\mathbb{R})$ and $W_{hy}\\in\\mathcal{M}_{n_y,n_h}(\\mathbb{R})$\n",
    "* The bias vectors $b_h\\in\\mathcal{M}_{n_h,1}(\\mathbb{R})$ and $b_y\\in\\mathcal{M}_{n_y,1}(\\mathbb{R})$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf612ed-db24-47dc-b9b7-971b260416c0",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Exercise:** Compute the gradients $\\frac{\\partial\\mathcal{L}}{\\partial W_{hy}}$ and $\\frac{\\partial\\mathcal{L}}{\\partial b_y}$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e593d8-3894-48f9-ad8f-192d7d692ff9",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601a37f-1d4c-4a43-b3e9-84fc63a55526",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "**Solution**: Recall that $y=W_{hy}h_n+b_y$, where $h_n$ is the final hidden state. Then:\n",
    "* $\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial W_{hy}} \n",
    "    = \\frac{\\partial\\mathcal{L}}{\\partial y}\\times\\frac{\\partial y}{\\partial W_{hy}}\n",
    "    = \\frac{\\partial\\mathcal{L}}{\\partial y} h_n^{\\top} \\,;$\n",
    "    \n",
    "* $\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial b_y} \n",
    "    = \\frac{\\partial\\mathcal{L}}{\\partial y}\\times\\frac{\\partial y}{\\partial b_y}\n",
    "    = \\frac{\\partial\\mathcal{L}}{\\partial y} \\,.$\n",
    "    \n",
    "_Note:_ Beware of the dimensions of these objects! These are not partial derivatives in $\\mathbb{R}$... -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94707e-9d88-4849-b782-d0f10f37edfd",
   "metadata": {},
   "source": [
    "Finally, we need the gradients for $W_{xh}$, $W_{hh}$, and $b_h$, which are used every step during the RNN. For example, for $W_{xh}$, we have \n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial W_{xh}} \n",
    "= \\sum_{t=0}^n \\frac{\\partial\\mathcal{L}}{\\partial h_t}\\frac{\\partial h_t}{\\partial W_{xh}} $$\n",
    "because changing $W_{xh}$ affects every $h_t$, which all affect $y$ and ultimately $\\mathcal{L}$. In order to fully calculate the gradient of $W_{xh}$, we will need to backpropagate through all time-steps, which is known as Backpropagation Through Time (BPTT).\n",
    "\n",
    "<img src=\"img/bptt.png\" width=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae05a60-f044-4da2-8fa9-543b380d337c",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Exercise:** At a given time step $t$, compute $\\frac{\\partial h_t}{\\partial W_{xh}}$, $\\frac{\\partial h_t}{\\partial W_{hh}}$ and $\\frac{\\partial h_t}{\\partial b_h}$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50eb5fa-99f7-443e-bfa5-3575478d2359",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd5b9a-c5d6-425b-93f8-fe3d889f172a",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "<!-- **Solution**: Recall that $h_t=\\tanh\\left( W_{xh}x_t + W_{hh}h_{t-1} + b_h \\right)$ and that $\\tanh^\\prime(x)=1-\\tanh^2(x)$. Then:\n",
    "\n",
    "* $\\displaystyle\\frac{\\partial h_t}{\\partial W_{xh}} = (1-h_t^2)\\,x_t^{\\top} \\,;$\n",
    "    \n",
    "* $\\displaystyle\\frac{\\partial h_t}{\\partial W_{hh}} = (1-h_t^2)\\,h_{t-1}^{\\top} \\,;$\n",
    "    \n",
    "* $\\displaystyle\\frac{\\partial h_t}{\\partial b_h} = (1-h_t^2) \\,.$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a41d3-a841-46a2-8d96-a3e7f2a38cc9",
   "metadata": {},
   "source": [
    "The last thing we need is $\\frac{\\partial\\mathcal{L}}{\\partial h_t}$. We can calculate it recursively:\n",
    "\n",
    "$$ \\forall t\\in[\\![0,n-1]\\!]\\,,\\quad  \\dfrac{\\partial\\mathcal{L}}{\\partial h_t} \n",
    "    = \\dfrac{\\partial\\mathcal{L}}{\\partial h_{t+1}}\\times\\dfrac{\\partial h_{t+1}}{\\partial h_t} \n",
    "    = W_{hh}^{\\top}\\, \\underbrace{\\left[(1-h_t^2)\\,\\dfrac{\\partial\\mathcal{L}}{\\partial h_{t+1}}\\right]}_{\\text{term-by-term multiplication}}\n",
    "    \\qquad\\text{and}\\qquad \n",
    "    \\dfrac{\\partial\\mathcal{L}}{\\partial h_n}=W_{hy}^{\\top}\\,\\frac{\\partial\\mathcal{L}}{\\partial y} \\;.$$\n",
    "    \n",
    "_Note:_ The recursion is _backward!_ We will implement BPTT starting from the last hidden state and working backwards, so we will already have $\\frac{\\partial\\mathcal{L}}{\\partial h_{t+1}}$ by the time we want to calculate $\\frac{\\partial\\mathcal{L}}{\\partial h_t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0611406-c339-481a-ae5b-53dbc0be775c",
   "metadata": {},
   "source": [
    "#### Back-Propagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95254000-416e-452c-a205-322c781a80eb",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Using the previous gradients computations, implement the back-propagation through time.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61da59f-3b87-4f20-a2d4-8943614283d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "class RNN:\n",
    "    # A Vanilla Recurrent Neural Network.\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = rd.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = rd.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = rd.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Perform a forward pass of the RNN using the given inputs.\n",
    "        Returns the final output and hidden state.\n",
    "        - inputs is an array of one-hot vectors with shape (input_size, 1).\n",
    "        '''\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.hs = { 0: h }\n",
    "        \n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.hs[i + 1] = h\n",
    "            \n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def backprop(self, d_y, learn_rate=2e-2):\n",
    "        '''    \n",
    "        Perform A backward pass of the RNN.    \n",
    "        - d_y (dL/dy) has shape (output_size, 1).    \n",
    "        - learn_rate is a float.    \n",
    "        '''    \n",
    "        n = len(self.inputs)\n",
    "\n",
    "        # Calculate dL/dWhy and dL/dby.\n",
    "        d_Why = ...\n",
    "        d_by = ...\n",
    "        \n",
    "        # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "        d_Whh = ...\n",
    "        d_Wxh = ...\n",
    "        d_bh = ...\n",
    "\n",
    "        # Calculate dL/dh for the last h.\n",
    "        d_h = ...\n",
    "\n",
    "        # Backpropagate through time.\n",
    "        for t in reversed(range(n)):\n",
    "            # An intermediate value: dL/dh * (1 - h^2)\n",
    "            tmp = ...\n",
    "\n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += ...\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += ...\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += ...\n",
    "            # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = ...\n",
    "            \n",
    "        # Clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "            \n",
    "        # Update weights and biases using gradient descent.\n",
    "        self.Whh ...\n",
    "        self.Wxh ...\n",
    "        self.Why ...\n",
    "        self.bh ...\n",
    "        self.by ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0e8da0ca-8352-4e1e-8ec2-9913816a59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/RNN_v2.py\n",
    "class RNN:\n",
    "    # A Vanilla Recurrent Neural Network.\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = rd.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = rd.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = rd.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Perform a forward pass of the RNN using the given inputs.\n",
    "        Returns the final output and hidden state.\n",
    "        - inputs is an array of one-hot vectors with shape (input_size, 1).\n",
    "        '''\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.hs = { 0: h }\n",
    "        \n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.hs[i + 1] = h\n",
    "            \n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def backprop(self, d_y, learn_rate=2e-2):\n",
    "        '''    \n",
    "        Perform a backward pass of the RNN.    \n",
    "        - d_y (dL/dy) has shape (output_size, 1).    \n",
    "        - learn_rate is a float.    \n",
    "        '''    \n",
    "        n = len(self.inputs)\n",
    "\n",
    "        # Calculate dL/dWhy and dL/dby.\n",
    "        d_Why = d_y @ self.hs[n].transpose()\n",
    "        d_by = d_y\n",
    "        \n",
    "        # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # Calculate dL/dh for the last h.\n",
    "        d_h = self.Why.transpose() @ d_y\n",
    "\n",
    "        # Backpropagate through time.\n",
    "        for t in reversed(range(n)):\n",
    "            # An intermediate value: dL/dh * (1 - h^2)\n",
    "            tmp = (1 - self.hs[t+1]**2) * d_h\n",
    "\n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += tmp\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) @ h_{t-1}^T\n",
    "            d_Whh += tmp @ self.hs[t].transpose()\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) @ x^T\n",
    "            d_Wxh += tmp @ self.inputs[t].transpose()\n",
    "            # Next dL/dh =  Whh^T @ [dL/dh * (1 - h^2)]\n",
    "            d_h = self.Whh.transpose() @ tmp\n",
    "            \n",
    "        # Clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "            \n",
    "        # Update weights and biases using gradient descent.\n",
    "        self.Whh -= learn_rate * d_Whh\n",
    "        self.Wxh -= learn_rate * d_Wxh\n",
    "        self.Why -= learn_rate * d_Why\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedbd00-7275-4403-990f-6fa1a2a55149",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edd547-6da6-4f91-a222-f889a1140af2",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write a helper function to process data with the RNN.</span>\n",
    "\n",
    "To do this, you can refer to the various tests we have carried out previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b2ed5-59a0-482d-ad7f-a3636c63721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data, backprop=True):\n",
    "    '''\n",
    "    Returns the RNN's loss and accuracy for the given data.\n",
    "    - data is a dictionary mapping text to True or False.\n",
    "    - backprop determines if the backward phase should be run.\n",
    "    '''\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for x, y in items:\n",
    "        inputs = ...\n",
    "        target = ...\n",
    "\n",
    "        # Forward\n",
    "        [...]\n",
    "        probs = ...\n",
    "\n",
    "        # Calculate loss / accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        num_correct += int(np.argmax(probs) == target)\n",
    "\n",
    "        if backprop:\n",
    "            # Build dL/dy\n",
    "            [...]\n",
    "\n",
    "            # Backward\n",
    "            [...]\n",
    "\n",
    "    return loss/len(data), num_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33acc766-bb03-4d82-8d32-037621d9de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/processData.py\n",
    "def processData(data, backprop=True):\n",
    "    '''\n",
    "    Returns the RNN's loss and accuracy for the given data.\n",
    "    - data is a dictionary mapping text to True or False.\n",
    "    - backprop determines if the backward phase should be run.\n",
    "    '''\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for x, y in items:\n",
    "        inputs = createInputs(x)\n",
    "        target = int(y)\n",
    "\n",
    "        # Forward\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        probs = softmax(out)\n",
    "\n",
    "        # Calculate loss / accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        num_correct += int(np.argmax(probs) == target)\n",
    "\n",
    "        if backprop:\n",
    "            # Build dL/dy\n",
    "            d_y = probs\n",
    "            d_y[target] -= 1\n",
    "\n",
    "            # Backward\n",
    "            rnn.backprop(d_y)\n",
    "\n",
    "    return loss/len(data), num_correct/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ce1e1-ea95-4543-84f1-17b798e5cc4b",
   "metadata": {},
   "source": [
    "Last, we can write the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "780f3764-56bc-4005-9772-34f25e1ddace",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 100\n",
      "Train:\tLoss 0.688 | Accuracy: 0.569\n",
      "Test:\tLoss 0.696 | Accuracy: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_246126/985038822.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
      "/tmp/ipykernel_246126/985038822.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 200\n",
      "Train:\tLoss 0.660 | Accuracy: 0.655\n",
      "Test:\tLoss 0.709 | Accuracy: 0.650\n",
      "--- Epoch 300\n",
      "Train:\tLoss 0.180 | Accuracy: 0.948\n",
      "Test:\tLoss 0.146 | Accuracy: 1.000\n",
      "--- Epoch 400\n",
      "Train:\tLoss 0.012 | Accuracy: 1.000\n",
      "Test:\tLoss 0.016 | Accuracy: 1.000\n",
      "--- Epoch 500\n",
      "Train:\tLoss 0.006 | Accuracy: 1.000\n",
      "Test:\tLoss 0.007 | Accuracy: 1.000\n",
      "--- Epoch 600\n",
      "Train:\tLoss 0.004 | Accuracy: 1.000\n",
      "Test:\tLoss 0.005 | Accuracy: 1.000\n",
      "--- Epoch 700\n",
      "Train:\tLoss 0.003 | Accuracy: 1.000\n",
      "Test:\tLoss 0.003 | Accuracy: 1.000\n",
      "--- Epoch 800\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.003 | Accuracy: 1.000\n",
      "--- Epoch 900\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.002 | Accuracy: 1.000\n",
      "--- Epoch 1000\n",
      "Train:\tLoss 0.001 | Accuracy: 1.000\n",
      "Test:\tLoss 0.002 | Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    train_loss, train_acc = processData(train_data)\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print('--- Epoch %d' % (epoch + 1))\n",
    "        print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "        test_loss, test_acc = processData(test_data, backprop=False)\n",
    "        print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5a81b-ad5d-492c-85eb-019037c79179",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Visualize the results of the training on the test data.</span>\n",
    "\n",
    "You will use the same color code as for the visualization of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186ecee-9f92-4604-afc6-e558175c55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "861f04e8-5a21-4d64-b3d6-f0beaffab2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mthis is happy\n",
      "\u001b[32mi am good\n",
      "\u001b[31mthis is not happy\n",
      "\u001b[31mi am not good\n",
      "\u001b[32mthis is not bad\n",
      "\u001b[32mi am not sad\n",
      "\u001b[32mi am very good\n",
      "\u001b[31mthis is very bad\n",
      "\u001b[31mi am very sad\n",
      "\u001b[31mthis is bad not good\n",
      "\u001b[32mthis is good and happy\n",
      "\u001b[31mi am not good and not happy\n",
      "\u001b[32mi am not at all sad\n",
      "\u001b[31mthis is not at all good\n",
      "\u001b[32mthis is not at all bad\n",
      "\u001b[32mthis is good right now\n",
      "\u001b[31mthis is sad right now\n",
      "\u001b[31mthis is very bad right now\n",
      "\u001b[32mthis was good earlier\n",
      "\u001b[31mi was not happy and not good earlier\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/scratch/coloredResults.py\n",
    "test_res = test_data\n",
    "\n",
    "for _, w in enumerate(test_res):\n",
    "    inputs = createInputs(w)\n",
    "    out, _ = rnn.forward(inputs)\n",
    "    res = softmax(out)<.5\n",
    "    res = bool(res[0])\n",
    "    test_res[w] = res\n",
    "    \n",
    "    if res:\n",
    "        print(Fore.GREEN + w)\n",
    "    else:\n",
    "        print(Fore.RED + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73994082-8f7d-4670-8228-81c138987c3f",
   "metadata": {},
   "source": [
    "# **Part II**: Study of the [IMDB](http://ai.stanford.edu/~amaas/data/sentiment/) Dataset\n",
    "\n",
    "<img src=\"img/imdb.png\" width=500>\n",
    "\n",
    "In this second part, we will train a classifier movie reviews in IMDB data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d5a77-5e48-411a-900d-02a3a099f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1241d4-2b21-4681-8961-fe79735df19e",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50de66b-11bb-489a-aaba-0199b2f8da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(start_char=1, oov_char=2, index_from=3)\n",
    "\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e757e60-06ff-4bd9-926a-762ec0cf6cbd",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "The commands below allow displaying a sample review and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f2089-488b-4ec7-9727-b1643a7887aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = rd.randint(len(X_train))\n",
    "\n",
    "print('---review number---')\n",
    "print(idx)\n",
    "\n",
    "print('\\n---review---')\n",
    "print(X_train[idx])\n",
    "\n",
    "print('\\n---label---')\n",
    "print(y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324158e-1507-4166-92ef-17f9754f5861",
   "metadata": {},
   "source": [
    "The review is stored as a sequence of integers. These are word IDs that have been pre-assigned to individual words, based on their frequencies: the more frequent a word, the lower the integer. The label is an integer (0 for negative, 1 for positive).\n",
    "\n",
    "To decode the review, we need to use the vocabulary, _i.e._, the dictionary that associates each word with its unique integer ID, which is available via the `get_word_index()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae1a01-aafe-424f-a258-cf317a3f2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_char = 0\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "\n",
    "word_to_idx = imdb.get_word_index()\n",
    "idx_to_word = {i+index_from: w for (w, i) in word_to_idx.items()}\n",
    "idx_to_word[pad_char] = \"[PAD]\"\n",
    "idx_to_word[start_char] = \"[START]\"\n",
    "idx_to_word[oov_char] = \"[OOV]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5f154-6256-4112-8462-d80be4a535cb",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write a function that displays a review in a readable form along with its label.</span>\n",
    "\n",
    "Keep a similar display to the one suggested above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2d771-5e3a-474e-a71c-d44c94080845",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "def decodeReview(idx):\n",
    "    '''\n",
    "    Converts the encoded idx-th review to human readable form.\n",
    "    Displays the review number, the review in words and the label\n",
    "    '''\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea724d3a-805c-4106-bce7-173f6470539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/decodeReview.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd2b39-656e-4a68-9886-d013c25f0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "decodeReview(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bb9e8-3dee-44fe-92a8-bc550315fc50",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** What is the proportion of positive reviews in the training dataset? And in the test dataset?</span>\n",
    "\n",
    "This question can be answered using a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836342a5-87df-4ec8-986f-563a81a2eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Proportion of positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb933fe-182e-472c-8450-2d70828b0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/positiveProportion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3466221-afef-4569-b938-3a5ae2fc9f55",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** How many different words does this database contain?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2e24c-bddd-41f6-85cc-a3938afab55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "vocab_size = ...\n",
    "print('%d unique words found' % vocab_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf6811-438e-4642-9b31-6e49536b97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/vocab_size.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cef1b-cdcf-45b0-86b8-210525b53f65",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** Are all reviews the same length? If not, what is their maximum length?</span>\n",
    "\n",
    "This question can be answered using an histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5ccf3-b7aa-4b0a-8aa9-cd80eb2265fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Lengths of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84721a47-77f6-472f-984a-f6e489c937f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/reviewsLengths.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5573faf-8645-44c4-bfb0-48f4b949bbcf",
   "metadata": {},
   "source": [
    "### Sequences Padding\n",
    "\n",
    "The reviews have a variable number of words, while the network has a fixed number of neurons. To get a fixed length input, we can simply truncate the reviews to a fixed number of words, say $\\texttt{max\\_words=200}$. To facilitate learning, we will also limit ourselves to the $\\texttt{vocab\\_size=10000}$ most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604b375-3eb0-4785-a96a-076088893e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1f3ad-6907-4a6b-9fb8-c64828665dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 200\n",
    "vocab_size = 10000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(start_char=1, oov_char=2, index_from=3, num_words = vocab_size)\n",
    "\n",
    "X_train_pad = sequence.pad_sequences(X_train, value=0, padding='post', maxlen=max_words)\n",
    "X_test_pad = sequence.pad_sequences(X_test, value=0, padding='post', maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e95cf-98a2-439b-bd18-1c6ffe9bd09b",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Check that the size of the reviews is now equal to $\\texttt{max\\_words}$ for each of them.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173e230-e8aa-488c-99cd-05b6cec5c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Lengths of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d19b7-f34d-40ec-9193-509423364404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/paddingLengths.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d855fcf-62ed-4ee9-8920-acadd29156b4",
   "metadata": {},
   "source": [
    "Let us see the effect of padding and truncation at the most frequent words on the previously displayed idx-th review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d2e49-d0d9-4e71-806c-d17872c48c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "decodeReview(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d6fdc4-fee5-4070-80c8-0fe5b8e07399",
   "metadata": {},
   "source": [
    "## RNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbc185-fa3c-48fd-b2d8-d9fdda80177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, SimpleRNN, LSTM, Dense, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6200be4-1088-4b0b-b37b-e512ef8ea133",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Design a RNN model for sentiment analysis.</span>\n",
    "\n",
    "The first layer must be an [`Embedding`](https://keras.io/api/layers/core_layers/embedding/) layer. To prevent gradient vanishing, choose a suitable recurrent network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040f769-e436-46a9-8251-6ad95a8f5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "embedding_size = 32\n",
    "\n",
    "rnn = Sequential(name=\"RNN\")\n",
    "rnn.add(Embedding(vocab_size, embedding_size))  #, input_length=max_words\n",
    "[...]\n",
    "\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503022c-2e7a-4df3-80eb-7ed88a7f67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/rnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790cd29-8efe-4323-8d8e-636b552825de",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Performing the learning.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0d5e2-9a27-4726-bd1d-454f225cdb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 8\n",
    "\n",
    "X_valid, y_valid = X_train_pad[:batch_size], y_train[:batch_size]\n",
    "X_train_rnn, y_train_rnn = X_train_pad[batch_size:], y_train[batch_size:]\n",
    "\n",
    "\n",
    "rnn.compile(loss=..., \n",
    "             optimizer=..., \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_rnn = rnn.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99f0b1-e4bd-4ed7-acbf-6f3fec9be06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/rnnTraining.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a15083-b62d-4f38-b9f0-d20d61eab224",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Visualize the learning process.</span>\n",
    "\n",
    "Write a function that allows to represent on two different figures the accuracy on one hand, and the loss on the other hand, each for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e2881-62e5-45f4-89b7-b37903fa9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "def plotTraining(history):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c3829-0d53-4e1c-bc06-3308c4d4a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/plotTraining.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d4c15-7cbe-4c9c-882c-f343c8215dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTraining(history_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237b8c5-61bf-48bc-9ee5-7da2c5fb0ed1",
   "metadata": {},
   "source": [
    "## Bidirectional RNN\n",
    "\n",
    "As defined, this network introduces a causal structure into the data. Also, for text processing, we often prefer a bidirectional network. To do this, we can use the [`Bidirectional`](https://keras.io/api/layers/recurrent_layers/bidirectional/) command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee2e3c-588b-45f4-abff-f59217850130",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "bi_rnn = Sequential(name=\"Bidirectional_RNN\")\n",
    "bi_rnn.add(Embedding(vocab_size, embedding_size))\n",
    "bi_rnn.add(Bidirectional(LSTM(int(.5*embedding_size))))  ### NEW ###\n",
    "bi_rnn.add(Dropout(0.1))\n",
    "bi_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(bi_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a81c21-bed6-4f9d-b8e8-c2f1cb251a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 8\n",
    "\n",
    "X_valid, y_valid = X_train_pad[:batch_size], y_train[:batch_size]\n",
    "X_train_rnn, y_train_rnn = X_train_pad[batch_size:], y_train[batch_size:]\n",
    "\n",
    "\n",
    "bi_rnn.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_bi_rnn = bi_rnn.fit(X_train_rnn, \n",
    "                    y_train_rnn, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epochs)\n",
    "\n",
    "plotTraining(history_bi_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023b1b0-0e6d-4d76-bf53-1abb53b34dad",
   "metadata": {},
   "source": [
    "Thanks to the $\\texttt{return\\_sequences}$ option, we can easily stack several RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b9883-309f-40b8-99c5-b1706115dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "bi2_rnn = Sequential(name=\"Double_Bidirectional_RNN\")\n",
    "bi2_rnn.add(Embedding(vocab_size, embedding_size))\n",
    "bi2_rnn.add(Bidirectional(LSTM(int(.5*embedding_size), return_sequences = True)))\n",
    "bi2_rnn.add(Bidirectional(LSTM(int(.5*embedding_size), return_sequences = False)))\n",
    "bi2_rnn.add(Dropout(0.1))\n",
    "bi2_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(bi2_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7ef75-0bcf-4cce-b16f-962448b79d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 8\n",
    "\n",
    "X_valid, y_valid = X_train_pad[:batch_size], y_train[:batch_size]\n",
    "X_train_rnn, y_train_rnn = X_train_pad[batch_size:], y_train[batch_size:]\n",
    "\n",
    "\n",
    "bi2_rnn.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_bi2_rnn = bi2_rnn.fit(X_train_rnn, \n",
    "                    y_train_rnn, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epochs)\n",
    "\n",
    "plotTraining(history_bi2_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1239e-fc01-4e3e-80f4-b90cf13449b6",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ffb9e-f111-421b-b807-2d9e7108662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d910c-b6f3-4190-bf04-64b7cd2bfdd0",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Compare the confusion matrices for the three models proposed above.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9feee-09c1-4900-96a0-4af1f97d8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Compare the confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60fa15-4e03-4779-939d-21543df46dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/confusion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d183f7d-c21b-4417-ab91-102b43e604bd",
   "metadata": {},
   "source": [
    "## MLP for Sentiment Analysis\n",
    "\n",
    "Just to be sure of the usefulness of a recurrent network, we decide to test a \"simple\" perceptron on the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c556d-97ab-4d2d-8471-4feb3a050909",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Compare the above results with those of an MLP. Conclude.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38429f1f-d6d2-4111-8a23-4dc706e9c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Comparison with a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7eb79-76ae-4a5a-89fe-39197ef46b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2b16d-f333-49f6-b0b7-7daef40cba7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2541478-c6fa-4d0b-bbfb-3ddc821ca33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
